---
title             : "Expectations bias moral evaluations"
shorttitle        : "Expectations bias moral evaluations"

author: 
  - name          : "Derek Powell"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Stanford University, Department of Psychology, 450 Serra Mall, Stanford, CA, 94305"
    email         : "derekpowell@stanford.edu"
  - name          : "Zachary Horne"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Arizona State University"

authornote: |
  Derek Powell, Department of Psychology, Stanford University; Zachary Horne, School of Social and Behavioral Sciences, Arizona State University.
  
  The authors would like to acknowledge the help of Larisa Hussak, Keith Holyoak, Alan Fiske, Hongjing Lu, John Hummel, Andrei Cimpian, and Ellen Markman for their comments and support.
  
  Both authors jointly conceived of the project and theory, designed and conducted experiments, and analyzed results. D.P. developed the formal aspects of the theory. 

abstract: |
  When we learn of tragic events--a violent killing or a terrorist attack--we are mournful and often driven to action. But responses to these events can differ substantially depending on the context in which they occur. When tragedy takes us by surprise, we are shocked and outraged, but when it strikes in unstable regions or rough neighborhoods, we are often resigned to feel that “these things happen.” Here we explain the role that expectations seem to play in moral evaluation by modeling evaluations as the comparison of the state of the world before and after an event. According to this theory, expectations influence evaluations through information-theoretic principles: less expected events do more to inform us about the state of the world than do more expected events. Consistent with this theory, in two preregistered studies we found that people’s judgments of morally-significant events were affected by the prior probability of that event. People were more upset about an event that was unexpected (e.g., a robbery at a clothing store) than an event that was more more probable (e.g., a robbery at a convenience store). This bias threatens to impose a pernicious cycle: People living in poverty, geo-politically unstable regions, and other dangerous circumstances have the greatest need for assistance and concern. However, for these very reasons, it is often unsurprising when they are harmed. Consequently, the influence of expectations on moral evaluations threatens to reduce observers’ concern for those most in need.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Evaluation; Moral judgment; Information theory"
wordcount         : "X"

bibliography      : ["wiw-biblio.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output:
  papaja::apa6_pdf:
    includes:
      after_body: "appendix.tex"
---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r load data, include=FALSE}
# analysis

library(tidyverse)

d_raw <- read.csv("../data/WIW+Judgment+Task+Preregistered_March+30%2C+2018_11.17.csv")

d_wide <- d_raw %>%
  filter(DistributionChannel=="anonymous") %>%
  rename(duration = Duration..in.seconds.) %>%
  select(workerId, age, sex, attn, duration, contains("_trial"), -contains("_trial_")) %>%
  filter(Check_trial == 0, attn == 1) %>%
  rename(
    Corruption_trial = corruption_trial,
    Clerk_trial = clerk_trial,
    Flattire_trial = flattire_trial,
    Rape_trial = rape_trial) %>%
  mutate(sex = if_else(sex == 1, "male", "female"))

reverse_items <- c( # sadly, only 6 of 10 items were reversed! :(
  "Cancer_trial",
  "Heater_trial",
  "Flattire_trial",
  "Clerk_trial",
  "Corruption_trial",
  "Rape_trial"
)

d_tidy <- d_wide %>%
  gather(trial, response, Earthquake_trial:BigDiff5_trial) %>%
  mutate(trial_type = ifelse(grepl("[0-9]", trial), "filler", "experimental")) %>%
  mutate(trial_type = ifelse(grepl("BigDiff",trial), "filler-diff", trial_type)) %>%
  mutate(likely_2nd = ifelse((trial %in% reverse_items), 1, 0))

d <- d_tidy %>%
  filter(trial_type == "experimental", trial != "Check_trial") %>%
  mutate(response = ifelse(likely_2nd==1, (1-response), response)) %>%
  mutate(response = abs(response-1)) %>%
  mutate(trial = gsub("_trial","",trial))

d2_raw <- read.csv("../data/WIW+Judgment+Task+Preregistered+-+Likert_April+20%2C+2018_17.08.csv")

d2_wide <- d2_raw %>%
  filter(DistributionChannel=="anonymous") %>%
  rename(duration = Duration..in.seconds.) %>%
  select(workerId, age, sex, attn, duration, contains("_trial"), -contains("_trial_")) %>%
  filter(
    Check1_trial == -1, 
    Check2_trial == -2, 
    Check3_trial == 1, 
    attn == 1,
    !is.na(workerId)
    ) %>%
  rename(
    Corruption_trial = corruption_trial,
    Clerk_trial = clerk_trial,
    Flattire_trial = flattire_trial,
    Rape_trial = rape_trial) %>%
  mutate(sex = if_else(sex == 1, "male", "female"))

# normal = more likely is 1
# reverse = less likely is 1

reverse_items2 <- c(
  "Cancer_trial",
  "Heater_trial",
  "Flattire_trial",
  "Clerk_trial",
  "Corruption_trial",
  "Rape_trial",
  "Earthquake_trial",
  "Fastfood_trial",
  "Police_trial"
)

d2_tidy <- d2_wide %>%
  select(-Check1_trial, -Check2_trial, -Check3_trial) %>%
  gather(trial, response, Tornado_trial:BigDiff5_trial) %>%
  mutate(trial_type = ifelse(grepl("[0-9]", trial), "filler", "experimental")) %>%
  mutate(trial_type = ifelse(grepl("BigDiff",trial), "filler-diff", trial_type)) %>%
  mutate(likely_2nd = ifelse((trial %in% reverse_items2), 1, 0))

d2 <- d2_tidy %>%
  filter(trial_type == "experimental") %>%
  mutate(response = ifelse(likely_2nd==1, response*-1, response)) %>%
  mutate(response = -1*response) %>%
  mutate(response = response + 3) %>%
  mutate(trial = gsub("_trial","",trial))
```

```{r compute descriptives, include=FALSE}
gender_exp1 <- d %>% 
  group_by(workerId) %>% 
  summarize(sex=first(sex)) %>% 
  ungroup() %>% 
  count(sex) %>% 
  spread(sex, n)

gender_exp2 <- d2 %>% 
  group_by(workerId) %>% 
  summarize(sex=first(sex)) %>% 
  ungroup() %>% 
  count(sex) %>% 
  spread(sex, n)

age_exp1 <- d %>% 
  group_by(workerId) %>% 
  summarize(age=first(age)) %>% 
  ungroup() %>% 
  summarize(age = median(age)) %>% 
  .$age

age_exp2 <- d2 %>% 
  group_by(workerId) %>% 
  summarize(age=first(age)) %>% 
  ungroup() %>% 
  summarize(age = median(age)) %>% 
  .$age
```

```{r fit brms models, include=FALSE}
library(brms)
devtools::source_gist(id = "f1994c0f8325abbc5d300600744af39d", filename="cbrm.R")

fit_exp1 <- cbrm(
  response ~ 0 + intercept + trial + (1|workerId), # another model adds likely_2nd
  data = d,
  family = bernoulli(),
  prior = prior(normal(0,3), class="b"),
  control = list(adapt_delta = .80),
  cores = parallel::detectCores(),
  sample_prior = TRUE,
  iter = 2000,
  file = "brms_models/fit_exp1.rds"
  )

fit_exp2 <- cbrm(
  response ~ trial + (1|workerId), # another model adds likely_2nd
  data = d2,
  family = cumulative(),
  prior = prior(normal(0,3), class="Intercept"),
  control = list(adapt_delta = .80),
  cores = parallel::detectCores(),
  sample_prior = TRUE,
  iter = 2000,
  file = "brms_models/fit_exp2.rds"
  )

fit_exp1rand <- cbrm(
  response ~ 0 + intercept + (1|trial) + (1|workerId), # another model adds likely_2nd
  data = d,
  family = bernoulli(),
  prior = prior(normal(0,3), class="b"),
  control = list(adapt_delta = .80),
  sample_prior = TRUE,
  cores = parallel::detectCores(),
  iter = 2000,
  file = "brms_models/fit_exp1rand.rds"
  )

fit_exp2rand <- cbrm(
  response ~ (1|trial) + (1|workerId), # another model adds likely_2nd
  data = d2,
  family = cumulative(),
  prior = prior(normal(0,3), class="Intercept"),
  control = list(adapt_delta = .80),
  cores = parallel::detectCores(),
  sample_prior = TRUE,
  iter = 2000,
  file = "brms_models/fit_exp2rand.rds"
  )
```

```{r hypothesis tests, include=FALSE}
print_bf <- function(hypothesis) {
  
  bf <- hypothesis$hypothesis$Evid.Ratio
  
  if (is.na(bf)) {
    return("< .001")
  }
  else if (bf < .001) {
    return("< .001")
  }
  else if (bf > 1000) {
    return("> 1000")
  }
  else {
    return(paste("=", as.character(bf)))
  }
}

coefs_exp1 <- broom::tidy(fit_exp1rand) %>%
  filter(term=="b_intercept")

# coefs_exp2 <- broom::tidy(fit_exp2) %>%
#   filter(term=="b_intercept")

bf_exp1 <- hypothesis(fit_exp1rand, "intercept = 0")
# bf_exp1 <- h1$hypothesis$Evid.Ratio

bf_exp2 <- hypothesis(fit_exp2rand, "-Intercept[2] = Intercept[3]")
# bf_exp2 <- h2$hypothesis$Evid.Ratio
```

```{r make figures, include=FALSE}
fig1 <- plot(marginal_effects(fit_exp1, "trial"))
fig_exp1 <- fig1$trial$data %>%
  ggplot() +
  aes(x = reorder(trial, -estimate__), y = estimate__, ymin = lower__, ymax = upper__) +
  geom_errorbar(width=.25) +
  geom_point(shape=17, size = 2) +
  coord_cartesian(ylim=c(0,1)) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "grey") +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    aspect.ratio = .75,
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = .7)
    ) +
  labs(
    x = "Trial",
    y = "Prob. less expected\njudged more upsetting"
  ) 
# library(gridExtra)
# grid.arrange(ggplotGrob(fig_exp1))

ggsave("fig1.eps",fig_exp1, height=8.7, width=8.7, unit="cm")
```

```{r fig2, include=FALSE}

fig2_cont <- marginal_effects(fit_exp2, "trial", plot=FALSE)
fig2 <- plot(marginal_effects(fit_exp2, "trial", ordinal=TRUE))

plt_cont <- fig2_cont$trial %>%
  ggplot() +
  aes(x = reorder(trial, -estimate__), y = estimate__, ymin = lower__, ymax = upper__) +
  geom_errorbar(width=0) +
  geom_point(shape=17, size = 1) +
  coord_cartesian(ylim=c(2,4)) +
  geom_hline(yintercept = 3, linetype = "dashed", color = "grey") +
  theme_bw() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    aspect.ratio = 10/16,
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = .7)
  ) +
  labs(
    x = "Trial",
    y = 'Upset Judgments'
  ) 

plt_heatmap <- fig2$trial$data %>%
  left_join(
    fig2_cont$trial %>% 
              rename(mean_estimate = estimate__) %>%
              select(trial, mean_estimate)
            , by = "trial"
    ) %>%
  ggplot(aes(y = cats__, x = reorder(trial, -mean_estimate), fill = estimate__)) +
  geom_tile(color="white") + 
  scale_fill_viridis_c(
    option="B", 
    limits=c(0,1), 
    breaks = c(0, .25, .5, .75, 1), 
    labels= c(0, .25, .5, .75, 1),
    guide = guide_colorbar(
      title.position="top", 
      barheight = unit(.5, "cm")
      )
    ) +
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    # aspect.ratio = 1,
    legend.position = "top",
    legend.key.width = unit(.75,"cm"),
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = .7)
  ) +
  labs(
    x = "Trial",
    y = "Response",
    fill = "Probability"
  ) +
  coord_fixed(ratio=1)

library(gtable)
g2 <- ggplotGrob(plt_cont + theme(
  axis.title.y = element_text(margin = margin(t=0,b=.5,l=0,r=.25,unit="cm")) # this is a kludge to get shared axis
))
g3 <- ggplotGrob(plt_heatmap + theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  axis.text.x = element_blank()
  ))

g <- rbind(g3, g2, size = "last")
# grid.arrange(g)

library(gridExtra)

ggsave("fig2.eps", grid.arrange(g), height=11.4, width=8.7, units = "cm")
```

When we learn of tragic events--a friend's loss of a loved-one, a violent killing, or a terrorist attack--we are mournful, concerned, angry, and driven to action. But our responses to these events differ depending on the context in which they occur [e.g., @Trope2010]. In particular, our expectations play an important role in our reactions to events. This seems true of the ordinary: when a friend's loved-one passes, we often ask, "was it sudden?"; the dramatic: a murder in a sleepy town prompts a very different reaction than a similar crime in a major city; and even the catastrophic: acts of terrorism seem to evoke more outrage, horror, and concern when they occur in peaceful rather than unstable regions. For instance, the 2015 Paris attacks were felt around the world. Yet, most of those mourning had little to say 15 hours earlier, when another tragic attack killed at least 43 people in Beirut [@Graham2015]. Likewise, stories of the Virginia Tech massacre, which killed 32 people, wildly overshadowed coverage of attacks killing nearly 200 people in Iraq--one of the bloodiest days since the 2003 invasion [@Greenslade2007]. In each of these situations, the surprise elicited by an event appears to have affected people’s reactions to it. In some contexts, harm is tragic. In others, “these things happen.” 

Although this pattern seems so intuitive as to be obvious, no reasonable code of ethics would justify these judgments, nor do any moral psychological frameworks explicate the role of expectations in moral evaluations. Outside of the domain of morality, everyday experiences and controlled experiments show that people’s evaluations of events unambiguously depend on their expectations about those events. There is a special thrill to having one’s expectations exceeded and disappointment when events fail to meet expectations: we root for the underdog, hold surprise parties, and foreshadow bad news to ease its delivery [@Bell1985]. Indeed, in a controlled study, Mellers and colleagues [-@Mellers1997] found that expectations influenced affective reactions during a gambling task: Participants were often more excited by an small unexpected gain (e.g., winning \$30 with 10% probability) than they were by a larger but more expected gain (e.g., winning \$60 with 90% probability) [also see @Shepperd2002]. 

It seems intuitive that surprise would shape people’s reactions to parties, sporting events, and games of chance. However, it seems wholly inappropriate that surprise would shape people’s reactions to events with serious moral implications. Tragedies are tragic, whether expected or not, and failure to recognize this could lead to tacit endorsement of the pain and suffering of others wherever pain and suffering are the status quo. If expectations truly do influence our reactions to morally-significant world events, as intuition and anecdote seem to suggest, there are potentially serious social consequences. To demonstrate the plausibility of this suggestion, we develop a theoretical account of how expectations influence evaluations, not as a strange quirk or uniquely human bias, but instead as a fundamental component of how people evaluate events. 

Events, as we will conceive of them, are changes in the state of the world--transitions from one state to another. For this reason, we construe event evaluation as a comparison between the states of the world before and after the event. Similar comparison processes play crucial roles in many theories of decision-making [e.g., @Bell1985; @Gul1991; @Loomes1986; @Mellers1997]. For instance, in Prospect Theory [@Kahneman1979; @Tversky1992], the utility of a prospect is evaluated by comparison to a current reference point. Our proposal is similar: we argue that expectations can set the reference points against which people compare future outcomes, so that assigning value to an event consists in comparing the state of the world following the event to the state of the world just prior to the event. Further, we assume that people’s knowledge of the world is uncertain, so we represent the states of the world probabilistically. This means that a person’s model for the state of the world is simply their expectations about what has happened, what is happening, and what will happen in the future. A direct implication of this proposal is that the evaluation of an event is linked to what is learned about the world as a result of that event’s occurrence. That is, we evaluate events positively or negatively to the extent that they inform our understanding of the world in positive or negative ways.

One striking implication of this account is that it provides an information-theoretic route to understanding how expectations directly influence evaluation. A fundamental insight of Information Theory is that the information carried by an event is a function of its prior probability; low probability events carry more information than high probability events [@Shannon1948]. We propose that people learn more about the state of the world when their expectations are violated by shocking world events as compared to when they are affirmed by less surprising events. For example, compared to a bombing in Lebanon, if a bombing occurs in Paris, a less expected location, we learn that the world is more dangerous than we had previously believed. Although this suggestion is novel in the moral domain, these points are widely accepted elsewhere. For instance, expectations have long been recognized as fundamental to models of human and animal learning [e.g., @Rescorla1972].

A more formal treatment of this theory establishes the potential for its generality across domains. Let the state of the world prior to some event be represented by a random variable, $X$. We'll assume $X$ is discrete, and that every realization $x$ represents some potential specific state of the world in all relevant aspects under consideration. An agent has some utility function $u$, that applies over the different realizations of $X$. Thus the expected utility of the present state of the world is:

\begin{equation*}
E_X[u(X)] = \sum_x u(x)p(x)
\end{equation*}

When we learn an event has occurred, we can represent this as an observation of a second, binary random variable, $S$. We can think of $S$ as a "signalling event" that informs us about about $X$ [following the formalism deployed by @Arrow1996]. That is, we assume there is a dependency between $S$ and $X$, so that observing $S$ informs us about the state of the world, $X$. For instance, reading a news report of a terror attack in Brussels informs us about the state of the world, telling us about what realizations of $X$ are no longer possible (e.g., any wherein it was a peaceful day in Brussels) and which are more or less likely. Conditioned on $S$, the expected utility of the world is:

\begin{equation*}
E_{X|S}[u(X)] = \sum_x u(x)p(x|s)
\end{equation*}

Our construal of event evaluation is as a comparison between states of the world following and prior to an event. Consequently, we define the value assigned to the signalling event, $V(S)$, as:

\begin{equation*}
V(S) = E_{X|S}[u(X)] - E_X[u(X)]
\end{equation*}

From this it can be shown that the value assigned to event $S$ is proportional to its prior probability (see Theoretical Formalization). Specifically, it is the difference in the expected utility under different values of $S$, weighted by the inverse of the prior probability of $S$. [^1]

\begin{equation*}
V(s=1)=(1-p(s=1))\sum_x u(x)p(x|s=1) - u(x)p(x|s=0)
\end{equation*}

This conclusion does not depend on the nature of the event in question nor on the utility function over events, but instead holds generally wherever we conceive of evaluation as a holistic comparison between uncertain states of the world. Consequently, this account predicts expectations will equally affect reactions to gambles with clearly specified values and probabilities [@Mellers1997], as well as real-world events with ill-defined values, where reasoners' expectations are defined by their existing beliefs and world knowledge. 

Given the potentially fundamental role that expectations play in event evaluation, and the potentially serious moral biases that may result, we sought to test empirically whether people’s evaluations of morally harmful events are affected by their expectations about those events. We conducted two preregistered studies examining the role of expectations in moral evaluations. In both studies, participants were presented with a series of trials in which they read brief descriptions of two different events and were asked to indicate which of the two events seemed more upsetting. In these trials, the two events were highly similar, but were manipulated so that they differed in their perceived prior probabilities: one event was more expected and one more unexpected. 

# Study Overview

We conducted two preregistered studies examining the role of expectations in moral evaluations (https://osf.io/86rsw/). Each study consisted of multiple trials where participants were presented brief descriptions of two events. Their task was to decide which of the two events was more upsetting. In Study 1, they made a two-alternative forced-choice, and in Study 2 they made their judgments using a five-point rating scale.  Both studies consisted of 16 experimental trials, five “equivalent” filler trials, and five “non-equivalent” filler trials.

The design of these studies was chosen with special consideration to two theoretical points. First, the information-theoretic account we have described makes a solely directional prediction--all else being equal, more surprising events will elicit stronger reactions than less surprising events. Asking participants to compare two similar events allowed us to test this directional prediction. Second, it is the informativeness of events, determined by one's prior expectations of the event, that we predicted would shape evaluations. Thus, reasoners should not have to be induced to hold expectations experimentally [as in experiments by @Mellers1997], but instead their own prior beliefs should be sufficient to shape their expectations and affect their evaluations. For this reason, we manipulated the prior probability of events by describing two otherwise identical events occurring in different contexts. We explicitly chose contexts that differed in ways that could not be accounted for by construal-level explanations [@Trope2010] or explanations based on biases towards one’s ingroup [@Brewer1999]. This both allowed us to test the role of expectations based on world knowledge and provided a more naturalistic set of items.

In experimental trials, the two events described identical harms, but one event was more expected and one more unexpected, as determined in a prior norming study (see SI).  For example, participants considered the following stimulus: 

* “A 30 year old man in California dies in an earthquake” [Expected]

* “A 30 year old man in Oklahoma dies in an earthquake” [Unexpected]

In each of these events, the harm to the victim is the same (death) but one event is more probable than the other, given the different likelihoods of earthquakes occurring in California versus Oklahoma. In addition to being more naturalistic, manipulating expectations through (otherwise irrelevant) contextual details helps ensure that the harm suffered by the victim is understood to be identical. In the context of gambles, it is possible to manipulate probabilities specifically and directly [@Mellers1997]. However, in the context of more realistic moral harms, any attempt to directly manipulate probabilities risks also changing participants’ perceptions of the degree of harm. For instance, suppose we describe two cancer patients, one with a 5% chance of survival and one with a 50% chance of survival. Even if they both ultimately meet the same fate, participants might fairly assume the patient with the lower chance of survival had a more serious illness, was more debilitated, suffered more severely, and so forth. In contrast, changing the context of the two events explains away these likelihood differences, better equating the events in other respects.

Both studies also included “equivalent” filler trials. In these trials, the two events differed in trivial contextual details that we did not expect would affect participants’ judgments. These filler trials were meant to prevent participants from becoming explicitly aware of the structure of the experimental trials. Finally, both studies included “non-equivalent” filler trials, the two events differed substantially in the degree of harm suffered by a victim, so that one event was expected to be seen as considerably more upsetting than the other. These trials were included to allow participants a chance to use the extremes of the response scale and to reduce any task demands that might drive them to make artificially fine-grained distinctions when generating their responses. All experimental materials for these studies are available as Supporting Information at https://osf.io/86rsw/.

## Exclusions 

In both studies, we excluded participants who failed to correctly answer attention-check questions or who indicated at the end of the study they had not taken their participation seriously. 

## Data Analysis 

We analyzed our data by performing Bayesian estimation using the probabilistic programming language Stan [@Carpenter2017]. We tested our preregistered predictions by computing Bayes Factors (i.e. $BF_{01}$) using the hypothesis function in the R package brms. Bayes Factors express the ratio of the probability of data under the null hypothesis to the probability of the data under an alternative hypothesis. Larger Bayes Factors indicate that the data are more likely under the null hypothesis (i.e., that the intercept is not different from zero) than the alternative hypothesis (i.e., that the intercept is different from zero), and vice versa. As Bayes Factors can be influenced by prior choices [@Gelman2017], we performed prior robustness checks to confirm that our conclusions would not depend substantially on the specification of the priors. 

To ensure the generality of our findings, we tested participants’ reactions to a variety of items and treated both participants and items as random effects throughout our analyses. The use of random effects over items allows us to consider these items as a sample from a larger population of possible items, and to generalize our findings to that wider population, much as we generalize from our sample of participants to the larger population [@Judd2012].

## Study 1 

### Participants 

A total of `r n_distinct(d_raw$workerId)` participants were recruited from Amazon’s Mechanical Turk work distribution website (mTurk). Of these, `r n_distinct(d$workerId)` passed attention checks and were included in the final analyses (`r gender_exp1$male` male, `r gender_exp1$female` female, median age = `r age_exp1` years old). All participants were paid $0.60 for their participation.

### Materials and procedure

Participants judged 16 experimental event-pairs and 10 equivalent filler event-pairs. On each trial (an example is shown above), participants were presented with the event-pair stimulus and had to judge which outcome was more upsetting in a two-alternative forced choice task. The two events were labeled “Outcome 1” and “Outcome 2” and the order, and whether Outcome 1 or 2 was the unexpected event, was randomized and counter-balanced. 

### Results and discussion

We predicted that people would judge that events where unexpected harm occurred were more upsetting than events where expected harm occurred. As indicated in Figure 1, we observed this trend. We estimated the magnitude of this effect by fitting a Bayesian logistic random effects model with participants’ responses as the dependent variable (1 = unexpected event is more upsetting; 0 = expected event is more upsetting) and random intercepts for item and subject, allowing us to generalize both to items and subjects we did not test. The intercept in this model represents the log-odds of selecting the unexpected event as being more upsetting. Thus, by examining the population-level intercept, we can test whether participants were biased toward selecting the unexpected event ($\beta$ > 0), the expected event ($\beta$ < 0), or were unbiased ($\beta$ = 0). We found that people were considerably more likely to think that unexpected events were more upsetting than events that were expected, Intercept = `r round(coefs_exp1$estimate, 3)`, 95% CI [`r round(coefs_exp1$lower, 3)`, `r round(coefs_exp1$upper, 3)`], $BF_{01}$ `r print_bf(bf_exp1)`.
Figure 1 shows participants' responses broken-down by individual items. Participants' bias toward selecting the unexpected event as more upsetting was consistent across the 16 experimental items.

(ref:fig1-caption)
Probability with which participants chose the less expected event as more upsetting across trials in Study 1. As predicted, participants exhibited a robust bias toward choosing the less expected event as more upsetting across all items. Error bars represent 95% credible intervals calculated from the hierarchical logistic regression model. See Supporting Information for full description of items.

```{r figure1, fig.cap="(ref:fig1-caption)"}
knitr::include_graphics("fig1.eps") 
```

## Study 2

In Study 1 we found that people’s judgments of events were biased by their expectations about those events. When forced to choose between two events, participants decided that unexpected events were more upsetting than expected events. In Study 2, we sought to test our hypothesis using a more conservative method: we provided participants with a more expressive response scale so that if they viewed the events under consideration as equally harmful, their responses could reflect their attitude.

### Participants

A total of `r n_distinct(d2_raw$workerId)` participants were recruited from Amazon’s Mechanical Turk work distribution website (mTurk). Of these, `r n_distinct(d2$workerId)` passed attention checks and were included in the final analyses (`r gender_exp2$male` male, `r gender_exp2$female` female, median age = `r age_exp2` years old). All participants were paid $0.60 for their participation.

### Materials and procedure {#s2-materials-procedure .unnumbered}

The materials in Study 2 were the same as those in Study 1. As in Study 1, on each trial of the study, participants were presented with a pair of actions labeled “Outcome 1” and “Outcome 2” and were asked, “Which outcome seems more upsetting?” In Study 2 participants made their rating on a five-point scale (Outcome 1 seems more upsetting, Outcome 1 seems a little more upsetting, neither seems more upsetting than the other, Outcome 2 seems a little more upsetting, Outcome 2 seems more upsetting). 

(ref:fig2-caption)
Participants’ responses by item in Study 2. Upper: heatmap plot displaying probability of choosing each response on the 1 to 5 scale across items. Lower: marginal expected response across items, with 95% credible interval calculated from the ordinal regression model. Note the scale has been constrained to show relevant region of results. In both plots, a response of 3 indicates indifference, responses below 3 indicate a bias to choose the more expected event as more upsetting, and responses above 3 indicate a bias to choose the less expected event as more upsetting. Similar to Study 1, participants showed a robust bias toward choosing the less expected event as more upsetting. Trials are sorted by response to aid readability.

```{r figure2, fig.cap="(ref:fig2-caption)", fig.align = "center", out.width = "\\textwidth", fig.pos = "!h"}
# plot(arrangeGrob(g))
knitr::include_graphics("fig2.eps") 
```

### Results and discussion

We predicted that participants would evaluate unexpected events as more upsetting than expected events. We estimated the magnitude of this effect by fitting a cumulative (ordinal) random effects model with participants’ scale responses as the dependent variable (1 to 5) and random intercepts for item and subject. This model produces four intercept coefficients, representing the cumulative log-odds of responses at each scale point or lower. For instance, the second coefficient represents the log-odds participants chose a 2 ("outcome 2 seems slightly more upsetting") or lower on the scale. Similarly, the third intercept coefficient represents the log-odds participants chose a 3 or lower on the scale. To examine the predicted effect, we compared the third intercept coefficient (representing the log-odds of choosing 4 or higher) to the inverse of the second intercept coefficient (thereby representing the log-odds *not* choosing a 3 or higher--i.e., choosing a 1 or 2), allowing us to test whether participants were more likely to choose the expected or unexpected event as being more upsetting in cases where they did not choose the neither option. This analysis indicated that people were more likely to think that events that were unexpected were more upsetting than events that were expected, $BF_{01}$ `r print_bf(bf_exp2)` (see supporting information for full model results). We observed this difference consistently across nearly every item (Figure 2). 

We note that the most common response was to view both events, which involve identical harms, as equivalently upsetting. However, across almost every item, participants showed a clear bias towards choosing unexpected events as more upsetting. Thus, Study 2 replicates the tendency to view unexpected moral events as more upsetting using a more conservative judgment task. 

### Discussion

Consistent with our predictions, manipulating expectations shaped evaluations across a range of morally-significant events. We found that people viewed unexpected harmful events as more upsetting than expected harmful events, even when the harm a victim suffered was the same. For instance, people judged that a robbery at a clothing store (unexpected) was more upsetting than a robbery at a convenience store (expected). Furthermore, these differences were large, systematic, and robust. Although there was variability in the situations that elicited the largest differences, our statistical approach treated trial as a random effect (see Methods and Results), allowing us to generalize our conclusions to items we did not test [@Judd2012].

We have argued that expectations shape our reactions to events because they determine how informative those events are in shaping our understanding of the world. Thus, we argue that well-grounded information theoretic principles can account for people’s divergent moral reactions to likely and unlikely events. 

In some contexts, it is clearly adaptive that the violation of our expectations would direct our attention to the events and situations about which our understanding is most in need of revision [@Rescorla1972]. For instance, an act of terrorism in a peaceful nation might be less expected because of our beliefs about the effectiveness of security forces, the stability of national relationships, and positive relationships among people groups in that nation. Therefore, a surprising act of terrorism casts doubt on our deeper understanding of the geopolitical situation, and thereby leads us to suspect similar acts are more likely to occur in the future. 

Yet, however well-grounded the underlying principles, there is a clear moral danger whenever expectations influence our moral concerns. In particular, these effects pose a significant threat when considering our reactions to humanitarian challenges brought on by poverty or regional instability. In these cases, the influence of expectations on moral evaluations threatens to impose vicious and morally pernicious cycles. For instance, people living in geo-politically unstable regions or in the developing world are often those who are most affected by terrorism, famine, and natural disasters, and are the very people in greatest need of assistance and concern from the world at-large. However, for precisely these reasons, it is often unsurprising when harm befalls people living in these circumstances. Thus the influence of expectations on moral evaluations threatens to reduce observer's concern for these victims most in need, making people less likely to donate time or money to aid victims, or to take political action to prevent further harms. The present research identifies this potentially pernicious moral bias and grounds it in information theoretic principles. Hopefully, by further understanding these cognitive dynamics, psychologists and policymakers can begin to find ways to reduce or counteract people's tendency to ignore the plight of those in most need of attention.


```{r render_appendix, include=FALSE}
render_appendix("appendix.Rmd")
```

\newpage

# References
```{r create_r-references}
r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
